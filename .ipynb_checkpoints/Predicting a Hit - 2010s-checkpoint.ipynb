{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Necessary-Libraries\" data-toc-modified-id=\"Import-Necessary-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Necessary Libraries</a></span></li><li><span><a href=\"#Exploratory-Data-Analyisis\" data-toc-modified-id=\"Exploratory-Data-Analyisis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory Data Analyisis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Investigate-Continuous-and-Categorical-Variables\" data-toc-modified-id=\"Investigate-Continuous-and-Categorical-Variables-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Investigate Continuous and Categorical Variables</a></span></li><li><span><a href=\"#Remap-categorical-values\" data-toc-modified-id=\"Remap-categorical-values-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Remap categorical values</a></span></li><li><span><a href=\"#Breakup-Dataframe-into-Hits-and-Non-Hits\" data-toc-modified-id=\"Breakup-Dataframe-into-Hits-and-Non-Hits-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Breakup Dataframe into Hits and Non Hits</a></span></li><li><span><a href=\"#Plotting-Distribution-of-continuous-variables\" data-toc-modified-id=\"Plotting-Distribution-of-continuous-variables-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Plotting Distribution of continuous variables</a></span></li><li><span><a href=\"#Plotting-Distribution-of-Categorical-Variables\" data-toc-modified-id=\"Plotting-Distribution-of-Categorical-Variables-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Plotting Distribution of Categorical Variables</a></span></li><li><span><a href=\"#Categorical-Results\" data-toc-modified-id=\"Categorical-Results-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Categorical Results</a></span></li></ul></li><li><span><a href=\"#Statistical-Tests\" data-toc-modified-id=\"Statistical-Tests-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Statistical Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Two-Sample-T-Tests\" data-toc-modified-id=\"Two-Sample-T-Tests-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Two Sample T Tests</a></span></li><li><span><a href=\"#Statistical-Test-Results\" data-toc-modified-id=\"Statistical-Test-Results-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Statistical Test Results</a></span></li></ul></li><li><span><a href=\"#Featuring-Engineering\" data-toc-modified-id=\"Featuring-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Featuring Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Dummy-Variables\" data-toc-modified-id=\"Create-Dummy-Variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create Dummy Variables</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#test-train-split-dataset\" data-toc-modified-id=\"test-train-split-dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>test train split dataset</a></span></li><li><span><a href=\"#KNN-Model\" data-toc-modified-id=\"KNN-Model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>KNN Model</a></span></li><li><span><a href=\"#Decisition-Tree\" data-toc-modified-id=\"Decisition-Tree-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Decisition Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Gridsearch-with-Random-Forest\" data-toc-modified-id=\"Gridsearch-with-Random-Forest-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Gridsearch with Random Forest</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>XGBoost</a></span></li><li><span><a href=\"#k-fold-Cross-Validation-using-XGBoost\" data-toc-modified-id=\"k-fold-Cross-Validation-using-XGBoost-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>k-fold Cross Validation using XGBoost</a></span></li><li><span><a href=\"#Combining-XGBoost-with-Gridsearch\" data-toc-modified-id=\"Combining-XGBoost-with-Gridsearch-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Combining XGBoost with Gridsearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-DataFrame-to-Numpy-Arrays\" data-toc-modified-id=\"Convert-DataFrame-to-Numpy-Arrays-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>Convert DataFrame to Numpy Arrays</a></span></li><li><span><a href=\"#Test-Model-on-Spotify-API-function\" data-toc-modified-id=\"Test-Model-on-Spotify-API-function-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>Test Model on Spotify API function</a></span></li></ul></li><li><span><a href=\"#Save-the-best-model\" data-toc-modified-id=\"Save-the-best-model-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Save the best model</a></span></li><li><span><a href=\"#Create-a-Confusion-Matrix\" data-toc-modified-id=\"Create-a-Confusion-Matrix-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Create a Confusion Matrix</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib Inline\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Datasets/dataset-of-10s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analyisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Investigate Continuous and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at which features are continous variables \n",
    "\n",
    "for col in data:\n",
    "    if data[col].nunique() > 12:\n",
    "        print(col, data[col].nunique())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of continuous variables\n",
    "continuous_variables = []\n",
    "for col in data:\n",
    "    if data[col].nunique() > 12 and col not in ['uri', 'target', 'track', 'artist']:\n",
    "        continuous_variables.append(col)\n",
    "        \n",
    "print(continuous_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at which features are categorical variables \n",
    "\n",
    "for col in data:\n",
    "    if data[col].nunique() <= 12:\n",
    "        print(col, data[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a list of categorical variables\n",
    "categorical_variables = []\n",
    "for col in data:\n",
    "    if data[col].nunique() <= 12 and col != 'target':\n",
    "        categorical_variables.append(col)\n",
    "print(categorical_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap in integers as standard pitch notation\n",
    "\n",
    "pitch_notation_dict = {0: 'C', 1: 'C-Sharp', 2: 'D', 3: 'D-Sharp', 4:'E', 5:'F', 6:'F-Sharp',\n",
    "                       7:'G', 8:'G-Sharp', 9:'A', 10:'B-Flat', 11:'B'\n",
    "    }\n",
    "\n",
    "data.key = data.key.replace(pitch_notation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap mode as major or minor key\n",
    "mode_dict = {1:'Major', 0:'Minor'}\n",
    "data['mode'] = data['mode'].replace(mode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakup Dataframe into Hits and Non Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at descriptive statistics for songs that are a hit and not a hit\n",
    "\n",
    "hit_data = data.loc[data.target == 1]\n",
    "not_a_hit_data = data.loc[data.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_a_hit_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Distribution of continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#let's look at the distribution of continuous variables for each class of songs\n",
    "    # Sort the dataframe by target\n",
    "hit_songs = data.loc[data['target'] == 1]\n",
    "non_hit_songs = data.loc[data['target'] == 0]\n",
    "\n",
    "for feat in continuous_variables:\n",
    "    sns.distplot(hit_songs[feat], label='Hit Songs' )\n",
    "    sns.distplot(non_hit_songs[feat], label='Not Hit Songs')\n",
    "    plt.title('Song Distribution By Class')\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#let's look at the distribution of continuous variables for each class of songs using a box plot\n",
    "    # Sort the dataframe by target\n",
    "hit_songs = data.loc[data['target'] == 1]\n",
    "non_hit_songs = data.loc[data['target'] == 0]\n",
    "\n",
    "for feat in continuous_variables:\n",
    "#     sns.boxplot(hit_songs[feat] )\n",
    "    sns.boxplot(non_hit_songs[feat])\n",
    "    plt.title('Song Distribution of Non Hit Songs')\n",
    "    plt.xticks(np.linspace(0, 1, 10))\n",
    "\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the distribution of continuous variables for each class of songs using a box plot\n",
    "    # Sort the dataframe by target\n",
    "hit_songs = data.loc[data['target'] == 1]\n",
    "non_hit_songs = data.loc[data['target'] == 0]\n",
    "\n",
    "for feat in continuous_variables:\n",
    "    sns.boxplot(hit_songs[feat] )\n",
    "#     sns.boxplot(non_hit_songs[feat])\n",
    "    plt.title('Song Distribution of Hit Songs')\n",
    "    plt.xticks(np.linspace(0, 1, 10))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Distribution of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the distribution of characteristics for songs not classified as a hit\n",
    "for feat in categorical_variables:\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    \n",
    "\n",
    "    hit_data[feat].value_counts().plot(kind='bar', color='b', position=1, width=.4, label='Hit Songs')\n",
    "    not_a_hit_data[feat].value_counts().plot(kind='bar', color='r', position=0, width=.4, label='Non-Hit Songs')\n",
    "    plt.legend(loc=1, )\n",
    "\n",
    "    plt.ylabel('Number of Songs')\n",
    "    plt.xlabel(feat)\n",
    "    plt.title('Hit Songs vs. Non-Hit Songs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the bar graphs for my categorical variables, you can clearly see which group is the dominant class when breaking down the value counts. Hit songs seem to favor the key of C in major scale and have a time signature of 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each continuous variable, check to see if the mean of each group (hit or Non Hit)\n",
    "#is the statistically different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Sample T Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H0: the mean of group A == mean of Group B; Ha: mean of group A != mean of group B\n",
    "\n",
    "for var in continuous_variables:\n",
    "    t_stat, p_value = ttest_ind(hit_data[var], not_a_hit_data[var])\n",
    "#     print(t_stat, p_value)\n",
    "   \n",
    "    print('\\n')\n",
    "    print(var, ': ', 'T_stat: ', t_stat, '/ P_value: ', p_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test Results\n",
    "After running a two sample T test on on each continuous variable between the two classes, I am able to reject the null hypothesis that the two groups are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuring Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables for time signature, key, mode\n",
    "\n",
    "data2 = pd.get_dummies(data=data, columns=['key', 'mode', 'time_signature'], \n",
    "               prefix=['key', 'mode', 'time_signature'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop cagtegorical variables\n",
    "# data2.drop(columns=['key', 'mode', 'time_signature'], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break up dataset into X and y\n",
    "\n",
    "X = data2.drop(['track', 'artist', 'uri', 'target'], axis=1)\n",
    "y = data2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break up dataset into X and y\n",
    "\n",
    "X_2 = data.drop(['track', 'artist', 'uri', 'target'], axis=1)\n",
    "y_2  = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale training and test set\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a knn model\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#fit scaled training set onto the model\n",
    "knn.fit(scaled_X_train, y_train)\n",
    "\n",
    "#make predictions with test set\n",
    "y_k_prediction1 = knn.predict(scaled_X_test)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, y_k_prediction1)\n",
    "accuracy = metrics.accuracy_score(y_test, y_k_prediction1)\n",
    "\n",
    "print('F1 Score: {}'.format(f1_score))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisition Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a baseline decision tree model\n",
    "dct = DecisionTreeClassifier()\n",
    "\n",
    "#fit training set to decision tree model\n",
    "dct.fit(X_train, y_train)\n",
    "\n",
    "#make a prediction\n",
    "\n",
    "y_dct_pred = dct.predict(X_test)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, y_dct_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_dct_pred)\n",
    "\n",
    "print('F1 Score: {}'.format(f1_score))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc = RandomForestClassifier(criterion='entropy', max_depth=8, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, y_rfc_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_rfc_pred)\n",
    "\n",
    "print('F1 Score: {}'.format(f1_score))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.predict(X_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets use gridsearch to find the best hyperparameters to use for our random forest model\n",
    "\n",
    "param_grid = {'criterion': ['entropy', 'gini']\n",
    "    'n_estimators': [450, 500, 550],\n",
    "              'max_depth': [2, 3, 4,5,6,7,8,9,10],\n",
    "              'max_features': ['auto', 'log2'],\n",
    "              'oob_score': ['True', 'False']\n",
    "              \n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_rfc = GridSearchCV(estimator=rfc, scoring='f1', param_grid=param_grid, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the best parameters\n",
    "gscv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets rerun our random forest model with the paramets gridsearch picked out\n",
    "\n",
    "\n",
    "rfc_bestparams = RandomForestClassifier(n_estimators=450, oob_score=False,\n",
    "    criterion='entropy', max_depth=9, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rfc_bestparams.fit(X_train, y_train)\n",
    "\n",
    "y_rfcbest_pred = rfc.predict(X_test)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, y_rfcbest_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_rfcbest_pred)\n",
    "\n",
    "print('F1 Score: {}'.format(f1_score))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xg_preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = metrics.f1_score(y_test, y_xg_preds)\n",
    "test_acc = metrics.accuracy_score(y_test, y_xg_preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate xgboost classifer\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "#define hyperparamters for gridsearch\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert DataFrame to Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert X_train to n_array\n",
    "X_array_train = pd.DataFrame.to_numpy(X_train)\n",
    "#convert y_train to n array\n",
    "\n",
    "y_array_train = pd.DataFrame.to_numpy(y_train)\n",
    "\n",
    "X_array_test =  pd.DataFrame.to_numpy(X_test)\n",
    "\n",
    "y_array_test = pd.DataFrame.to_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with training set\n",
    "gsearch1.fit(X_array_train, y_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions using best model\n",
    "\n",
    "prediction = gsearch1.best_estimator_.predict(X_array_test)\n",
    "gsearch1.best_estimator_.predict\n",
    "f1_score = metrics.f1_score(y_array_test, prediction)\n",
    "accuracy = metrics.accuracy_score(y_array_test, prediction)\n",
    "precision = metrics.precision_score(y_array_test, prediction)\n",
    "recall = metrics.recall_score(y_array_test, prediction)\n",
    "\n",
    "print('F1 Score: {}'.format(f1_score))\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the F1 score for all of my models, I can conclude that combining GridsearchCV and XGBoost produced the best results for the goal of this project. When deciding which metric I should be evaluating my model on, the f1 score seemed most appropriate. When curating a music playlist for a massive audience, it's important to be able to have a model that recognizes as many hits as possible while also identifying the songs that may not fit and therefore cause the listener to disengage from the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pickle list object\n",
    "\n",
    "\n",
    "pickle.dump(gsearch1.best_estimator_, open(\"spotify_api/model.pickle.dat\", \"wb\"))\n",
    "# model_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = open('spotify_api/model.pickle.dat', 'rb')\n",
    "xgboostmodel = pickle.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.45, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboostmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotify_api.fetch_data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spotify_api.fetch_data import*\n",
    "\n",
    "make_prediction('old town road', 'lil nas x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, prediction)\n",
    "# classes = ['Non-Hit Songs', 'Hit Songs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "group_names = ['TN', 'FP', 'FN', 'TP']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Values');ax.set_ylabel('Actual Values'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Hits', 'Non-Hits']); ax.yaxis.set_ticklabels(['Non-Hits', 'Hits'])\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylim([0,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.641px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
